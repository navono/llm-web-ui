name: llm-web-ui

networks:
  llm-network:
    driver: bridge

services:
  embeddings-transform:
    image: node:18-alpine
    container_name: llm-web-ui-embeddings-transform
    working_dir: /app
    command: node transform-embeddings.js
    volumes:
      - ./transform-embeddings.js:/app/transform-embeddings.js:ro
    env_file:
      - ../.env
    networks:
      - llm-network
    expose:
      - "9000"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: llm-web-ui-gateway
    ports:
      - "8080:8080"
    env_file:
      - ../.env
    volumes:
      # 直接挂载配置文件，使用 entrypoint 脚本处理环境变量
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf.template:ro
      - ./nginx/entrypoint.sh:/docker-entrypoint.d/40-envsubst-on-nginx-conf.sh:ro
      - ../.env:/app/.env:ro
    networks:
      - llm-network
    depends_on:
      - open-llm-vtuber
      - indextts2
      - embeddings-transform
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --quiet --tries=1 --spider http://localhost:8080/health || exit 1",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  open-llm-vtuber:
    image: open-llm-vtuber:latest
    container_name: llm-web-ui-vtuber
    env_file:
      - ../.env
    networks:
      - llm-network
    ports:
      - "12393:12393"
    volumes:
      # Configuration files
      - ./open-llm-vtuber/conf.yaml:/app/conf.yaml:ro
      - ./open-llm-vtuber/mcp_servers.json:/app/mcp_servers.json:ro
      # User data directories
      - ./open-llm-vtuber/avatars:/app/avatars
      - ./open-llm-vtuber/backgrounds:/app/backgrounds
      - ./open-llm-vtuber/characters:/app/characters
      - ./open-llm-vtuber/live2d-models:/app/live2d-models
      # Models (read-write temporarily to clean up)
      - ./open-llm-vtuber/models:/app/models
      # Logs (persistent)
      - ./open-llm-vtuber/logs:/app/logs
    environment:
      - HF_HOME=/app/models
      - MODELSCOPE_CACHE=/app/models
      # Uncomment to use Hugging Face mirror
      # - HF_ENDPOINT=https://hf-mirror.com
      # HTTP/HTTPS proxy settings
      - HTTP_PROXY=${PROXY_SERVER}
      - HTTPS_PROXY=${PROXY_SERVER}
      - NO_PROXY=${NO_PROXY:-localhost,127.0.0.1,172.17.0.1,172.18.0.1}
      - http_proxy=${PROXY_SERVER}
      - https_proxy=${PROXY_SERVER}
      - no_proxy=${NO_PROXY:-localhost,127.0.0.1,172.17.0.1,172.18.0.1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12393/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  indextts2:
    build:
      context: ..
      dockerfile: docker/Dockerfile.indextts2
      args:
        HTTP_PROXY: ${PROXY_SERVER}
        HTTPS_PROXY: ${PROXY_SERVER}
        NO_PROXY: ${NO_PROXY:-}
    image: indextts2:latest
    container_name: llm-web-ui-indextts2
    env_file:
      - ../.env
    networks:
      - llm-network
    ports:
      - "12234:12234"
    volumes:
      # 挂载整个 HuggingFace hub 目录，包含所有模型
      - "/mnt/f/data/HF_models/:/root/.cache/huggingface/"
      # 挂载 IndexTTS-2 模型目录
      - "/mnt/f/data/HF_models/hub/models--IndexTeam--IndexTTS-2:/app/models_cache"
      # 挂载音频提示文件目录
      - "/mnt/e/OneDrive/data_sync/audio_samples:/app/audio_prompts"
      # 输出目录（可选）
      - "./output:/app/output"
    environment:
      - PYTHONUNBUFFERED=1
      - CUDA_VISIBLE_DEVICES=0
      - PORT=12234
      # HTTP/HTTPS proxy settings
      - HTTP_PROXY=${PROXY_SERVER}
      - HTTPS_PROXY=${PROXY_SERVER}
      - NO_PROXY=${NO_PROXY:-localhost,127.0.0.1,172.17.0.1,172.18.0.1}
      - http_proxy=${PROXY_SERVER}
      - https_proxy=${PROXY_SERVER}
      - no_proxy=${NO_PROXY:-localhost,127.0.0.1,172.17.0.1,172.18.0.1}
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
